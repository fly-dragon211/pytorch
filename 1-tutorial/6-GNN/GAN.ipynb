{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# morvan_pytorch GAN\n",
    "https://morvanzhou.github.io/tutorials/machine-learning/torch/4-06-GAN/\n",
    "训练一个GAN画二次曲线，生成的过程如下图：\n",
    "\n",
    "![莫凡gif](https://morvanzhou.github.io/static/results/torch/4-6-1.gif)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0434702bb146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mG_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mprob_artist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0moptim_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mG_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0moptim_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\D_disk\\Anaconda3\\envs\\my_ml\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\D_disk\\Anaconda3\\envs\\my_ml\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "两次bakward()之前必须重新计算，否则梯度计算会叠加\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 64\n",
    "LR_G = 0.001  # learning rate for generator\n",
    "LR_D = 0.001  # learning rate for discriminator\n",
    "N_IDEAS = 5  # think of it as number of ideas for generating an art work\n",
    "ART_COMPONENTS = 15  # it could be the total point G can draw in the canvas\n",
    "PAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])\n",
    "\n",
    "\n",
    "def artist_works():\n",
    "    a = np.random.uniform(1, 2, size=BATCH_SIZE).reshape((-1, 1))\n",
    "    paintins = a * np.power(PAINT_POINTS, 2) + (a - 1)\n",
    "    paintins = torch.tensor(paintins, dtype=torch.float)\n",
    "    return paintins\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(N_IDEAS, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, ART_COMPONENTS),\n",
    ")\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(ART_COMPONENTS, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid(),  # the prbability that the art work is made by artist\n",
    ")\n",
    "\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=LR_G)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=LR_D)\n",
    "\n",
    "plt.ion()  # it is about continuous plotting\n",
    "\n",
    "for step in range(int(1e4)):\n",
    "    artist_paintings = artist_works()\n",
    "    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS, requires_grad=True)\n",
    "    G_paintings = G(G_ideas)\n",
    "    \n",
    "    prob_artist0 = D(artist_paintings)  # D tries to increase the probability\n",
    "    prob_artist1 = D(G_paintings.detach())  # D tries to decrease the probability\n",
    "    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1-prob_artist1))\n",
    "    optim_D.zero_grad()\n",
    "    D_loss.backward()\n",
    "    optim_D.step()\n",
    "    \n",
    "    prob_artist1 = D(G_paintings)  # D tries to decrease the probability\n",
    "    G_loss = torch.mean(torch.log(1-prob_artist1))\n",
    "    optim_G.zero_grad()\n",
    "    G_loss.backward()\n",
    "    optim_G.step()\n",
    "    \n",
    "    if step % 40 == 0:  # plotting\n",
    "        plt.cla()\n",
    "        plt.plot(PAINT_POINTS[0], G_paintings.data.numpy()[0], c='#4AD631', lw=3, label='Generated painting',)\n",
    "        plt.plot(PAINT_POINTS[0], 2 * np.power(PAINT_POINTS[0], 2) + 1, c='#74BCFF', lw=3, label='upper bound')\n",
    "        plt.plot(PAINT_POINTS[0], 1 * np.power(PAINT_POINTS[0], 2) + 0, c='#FF9359', lw=3, label='lower bound')\n",
    "        plt.text(-.5, 2.3, 'D accuracy=%.2f (0.5 for D to converge)' % prob_artist0.data.numpy().mean(), fontdict={'size': 13})\n",
    "        plt.text(-.5, 2, 'D score= %.2f (-1.38 for G to converge)' % -D_loss.data.numpy(), fontdict={'size': 13})\n",
    "        plt.ylim((0, 3));plt.legend(loc='upper right', fontsize=10);plt.draw();plt.pause(0.01)\n",
    "        \n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.uniform(1, 2, size=BATCH_SIZE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两次backward()必须要重新计算，否则计算的是二阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first bakward:  tensor([8., 1.])\n",
      "After second bakward:  tensor([16.,  2.])\n",
      "After third bakward but set the gradient zero:  tensor([8., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Function(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Function, self).__init__()\n",
    "    def forward(self, input):\n",
    "        output = 2*input[0]**2 + input[1]\n",
    "        return output\n",
    "\n",
    "func = Function()\n",
    "\n",
    "x= torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "y = func(x)\n",
    "y.backward(retain_graph=True)\n",
    "\n",
    "print('After first bakward: ', x.grad)\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print('After second bakward: ', x.grad)\n",
    "\n",
    "x.grad.zero_()\n",
    "y.backward(retain_graph=True)\n",
    "print('After third bakward but set the gradient zero: ', x.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ml",
   "language": "python",
   "name": "my_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
